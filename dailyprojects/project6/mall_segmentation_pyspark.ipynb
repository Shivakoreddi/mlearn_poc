{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac495ae",
   "metadata": {},
   "source": [
    "# üõçÔ∏è Mall Customer Segmentation with PySpark\n",
    "Generated 2025-05-06 02:16 UTC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc610676",
   "metadata": {},
   "source": [
    "## üîç 1Ô∏è‚É£¬†Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99cf356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The operation couldn‚Äôt be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "/Users/shiva/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMallSegmentation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m data = [(\u001b[32m1\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mMale\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m19\u001b[39m,\u001b[32m15\u001b[39m,\u001b[32m39\u001b[39m),(\u001b[32m2\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mFemale\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m21\u001b[39m,\u001b[32m16\u001b[39m,\u001b[32m81\u001b[39m),(\u001b[32m3\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mFemale\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m20\u001b[39m,\u001b[32m17\u001b[39m,\u001b[32m6\u001b[39m),(\u001b[32m4\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mMale\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m23\u001b[39m,\u001b[32m18\u001b[39m,\u001b[32m77\u001b[39m),(\u001b[32m5\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mFemale\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m31\u001b[39m,\u001b[32m19\u001b[39m,\u001b[32m40\u001b[39m),(\u001b[32m6\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mMale\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m22\u001b[39m,\u001b[32m20\u001b[39m,\u001b[32m76\u001b[39m),(\u001b[32m7\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mFemale\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m35\u001b[39m,\u001b[32m21\u001b[39m,\u001b[32m6\u001b[39m),(\u001b[32m8\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mMale\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m23\u001b[39m,\u001b[32m22\u001b[39m,\u001b[32m94\u001b[39m),(\u001b[32m9\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mFemale\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m64\u001b[39m,\u001b[32m23\u001b[39m,\u001b[32m3\u001b[39m),(\u001b[32m10\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mMale\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m30\u001b[39m,\u001b[32m24\u001b[39m,\u001b[32m72\u001b[39m)]\n\u001b[32m      6\u001b[39m cols = [\u001b[33m\"\u001b[39m\u001b[33mCustomerID\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mGender\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mAge\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mAnnualIncomeK\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mSpendingScore\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/pyspark/sql/session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/pyspark/context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/pyspark/context.py:201\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    203\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    204\u001b[39m         master,\n\u001b[32m    205\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m         memory_profiler_cls,\n\u001b[32m    216\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/pyspark/context.py:436\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/pyspark/java_gateway.py:107\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    108\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    109\u001b[39m         message_parameters={},\n\u001b[32m    110\u001b[39m     )\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    113\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MallSegmentation\").getOrCreate()\n",
    "\n",
    "data = [(1,\"Male\",19,15,39),(2,\"Female\",21,16,81),(3,\"Female\",20,17,6),(4,\"Male\",23,18,77),(5,\"Female\",31,19,40),(6,\"Male\",22,20,76),(7,\"Female\",35,21,6),(8,\"Male\",23,22,94),(9,\"Female\",64,23,3),(10,\"Male\",30,24,72)]\n",
    "cols = [\"CustomerID\",\"Gender\",\"Age\",\"AnnualIncomeK\",\"SpendingScore\"]\n",
    "df = spark.createDataFrame(data, cols)\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00402a08",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 2Ô∏è‚É£¬†Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c42ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Gender\", outputCol=\"GenderIdx\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "df = df.withColumn(\"IncomePerAge\", F.col(\"AnnualIncomeK\") / (F.col(\"Age\")+1e-5))\n",
    "df.select(\"Gender\",\"GenderIdx\",\"IncomePerAge\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388529e8",
   "metadata": {},
   "source": [
    "## üßπ 3Ô∏è‚É£¬†Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "feature_cols = [\"AnnualIncomeK\",\"SpendingScore\",\"GenderIdx\",\"IncomePerAge\"]\n",
    "vec_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\")\n",
    "df_vec = vec_assembler.transform(df)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "df_scaled = scaler.fit(df_vec).transform(df_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e70c7b",
   "metadata": {},
   "source": [
    "## ü§ñ 4Ô∏è‚É£¬†Model Development ‚Äì KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84cc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "km = KMeans(k=3, seed=42, featuresCol=\"features\", predictionCol=\"kmeans_label\")\n",
    "model = km.fit(df_scaled)\n",
    "df_km = model.transform(df_scaled)\n",
    "df_km.select(\"CustomerID\",\"kmeans_label\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aaabbc",
   "metadata": {},
   "source": [
    "## üîß 5Ô∏è‚É£¬†Hyperparameter Tuning (Silhouette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"pred\", metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
    "best_k, best_score = None, -1\n",
    "for k in range(2,7):\n",
    "    m = KMeans(k=k, seed=42, featuresCol=\"features\", predictionCol=\"pred\").fit(df_scaled)\n",
    "    score = evaluator.evaluate(m.transform(df_scaled))\n",
    "    print(f\"k={k}, silhouette={score:.3f}\")\n",
    "    if score > best_score:\n",
    "        best_k, best_score = k, score\n",
    "print(\"Best k:\", best_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89c90b",
   "metadata": {},
   "source": [
    "## ‚úÖ 6Ô∏è‚É£¬†Final Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9738b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final = KMeans(k=best_k, seed=42, featuresCol=\"features\", predictionCol=\"segment\").fit(df_scaled)\n",
    "df_final = final.transform(df_scaled)\n",
    "print(\"Silhouette:\", evaluator.evaluate(df_final))\n",
    "df_final.select(\"CustomerID\",\"AnnualIncomeK\",\"SpendingScore\",\"segment\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680b5f9",
   "metadata": {},
   "source": [
    "## üìä 7Ô∏è‚É£¬†Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pdf = df_final.select(\"AnnualIncomeK\",\"SpendingScore\",\"segment\").toPandas()\n",
    "sns.scatterplot(data=pdf, x=\"AnnualIncomeK\", y=\"SpendingScore\", hue=\"segment\", palette=\"Set2\")\n",
    "plt.title(f\"Mall Segments (k={best_k})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747b7f5",
   "metadata": {},
   "source": [
    "## üß† 8Ô∏è‚É£¬†Insights & Next Steps\n",
    "- High/low spenders clusters etc.\n",
    "- Try DBSCAN or BisectingKMeans for alternative clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
