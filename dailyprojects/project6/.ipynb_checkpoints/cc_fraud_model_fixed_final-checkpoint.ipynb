{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d912b46",
   "metadata": {},
   "source": [
    "# üîç ML Data Inspection Plan\n",
    "Generated 2025-04-18 02:15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e40fd6",
   "metadata": {},
   "source": [
    "# üîç CC Fraud Data Inspection Plan (Enriched)\n",
    "Generated 2025-04-22 04:42\n",
    "\n",
    "This notebook performs a thorough exploratory data analysis (EDA) and data cleaning for the credit card fraud detection dataset. Each section includes detailed explanations, expected outputs, and next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49328db",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 1\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c23ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb608fbd",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Load and Inspect Raw Data\n",
    "**Purpose:** Load the CSV file, verify its size and preview the first rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba2de9",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 2\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1bc3193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1296675, 23)\n",
      "Memory usage before optimization:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1296675 entries, 0 to 1296674\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count    Dtype  \n",
      "---  ------                 --------------    -----  \n",
      " 0   Unnamed: 0             1296675 non-null  int64  \n",
      " 1   trans_date_trans_time  1296675 non-null  object \n",
      " 2   cc_num                 1296675 non-null  int64  \n",
      " 3   merchant               1296675 non-null  object \n",
      " 4   category               1296675 non-null  object \n",
      " 5   amt                    1296675 non-null  float64\n",
      " 6   first                  1296675 non-null  object \n",
      " 7   last                   1296675 non-null  object \n",
      " 8   gender                 1296675 non-null  object \n",
      " 9   street                 1296675 non-null  object \n",
      " 10  city                   1296675 non-null  object \n",
      " 11  state                  1296675 non-null  object \n",
      " 12  zip                    1296675 non-null  int64  \n",
      " 13  lat                    1296675 non-null  float64\n",
      " 14  long                   1296675 non-null  float64\n",
      " 15  city_pop               1296675 non-null  int64  \n",
      " 16  job                    1296675 non-null  object \n",
      " 17  dob                    1296675 non-null  object \n",
      " 18  trans_num              1296675 non-null  object \n",
      " 19  unix_time              1296675 non-null  int64  \n",
      " 20  merch_lat              1296675 non-null  float64\n",
      " 21  merch_long             1296675 non-null  float64\n",
      " 22  is_fraud               1296675 non-null  int64  \n",
      "dtypes: float64(5), int64(6), object(12)\n",
      "memory usage: 1.0 GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>trans_date_trans_time</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>street</th>\n",
       "      <th>...</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>dob</th>\n",
       "      <th>trans_num</th>\n",
       "      <th>unix_time</th>\n",
       "      <th>merch_lat</th>\n",
       "      <th>merch_long</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01 00:00:18</td>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Banks</td>\n",
       "      <td>F</td>\n",
       "      <td>561 Perry Cove</td>\n",
       "      <td>...</td>\n",
       "      <td>36.0788</td>\n",
       "      <td>-81.1781</td>\n",
       "      <td>3495</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>1988-03-09</td>\n",
       "      <td>0b242abb623afc578575680df30655b9</td>\n",
       "      <td>1325376018</td>\n",
       "      <td>36.011293</td>\n",
       "      <td>-82.048315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-01 00:00:44</td>\n",
       "      <td>630423337322</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>Stephanie</td>\n",
       "      <td>Gill</td>\n",
       "      <td>F</td>\n",
       "      <td>43039 Riley Greens Suite 393</td>\n",
       "      <td>...</td>\n",
       "      <td>48.8878</td>\n",
       "      <td>-118.2105</td>\n",
       "      <td>149</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>1978-06-21</td>\n",
       "      <td>1f76529f8574734946361c461b024d99</td>\n",
       "      <td>1325376044</td>\n",
       "      <td>49.159047</td>\n",
       "      <td>-118.186462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-01 00:00:51</td>\n",
       "      <td>38859492057661</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>Edward</td>\n",
       "      <td>Sanchez</td>\n",
       "      <td>M</td>\n",
       "      <td>594 White Dale Suite 530</td>\n",
       "      <td>...</td>\n",
       "      <td>42.1808</td>\n",
       "      <td>-112.2620</td>\n",
       "      <td>4154</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>1962-01-19</td>\n",
       "      <td>a1a22d70485983eac12b5b88dad1cf95</td>\n",
       "      <td>1325376051</td>\n",
       "      <td>43.150704</td>\n",
       "      <td>-112.154481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-01 00:01:16</td>\n",
       "      <td>3534093764340240</td>\n",
       "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>45.00</td>\n",
       "      <td>Jeremy</td>\n",
       "      <td>White</td>\n",
       "      <td>M</td>\n",
       "      <td>9443 Cynthia Court Apt. 038</td>\n",
       "      <td>...</td>\n",
       "      <td>46.2306</td>\n",
       "      <td>-112.1138</td>\n",
       "      <td>1939</td>\n",
       "      <td>Patent attorney</td>\n",
       "      <td>1967-01-12</td>\n",
       "      <td>6b849c168bdad6f867558c3793159a81</td>\n",
       "      <td>1325376076</td>\n",
       "      <td>47.034331</td>\n",
       "      <td>-112.561071</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-01 00:03:06</td>\n",
       "      <td>375534208663984</td>\n",
       "      <td>fraud_Keeling-Crist</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>41.96</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>Garcia</td>\n",
       "      <td>M</td>\n",
       "      <td>408 Bradley Rest</td>\n",
       "      <td>...</td>\n",
       "      <td>38.4207</td>\n",
       "      <td>-79.4629</td>\n",
       "      <td>99</td>\n",
       "      <td>Dance movement psychotherapist</td>\n",
       "      <td>1986-03-28</td>\n",
       "      <td>a41d7549acf90789359a9aa5346dcb46</td>\n",
       "      <td>1325376186</td>\n",
       "      <td>38.674999</td>\n",
       "      <td>-78.632459</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 trans_date_trans_time            cc_num  \\\n",
       "0           0   2019-01-01 00:00:18  2703186189652095   \n",
       "1           1   2019-01-01 00:00:44      630423337322   \n",
       "2           2   2019-01-01 00:00:51    38859492057661   \n",
       "3           3   2019-01-01 00:01:16  3534093764340240   \n",
       "4           4   2019-01-01 00:03:06   375534208663984   \n",
       "\n",
       "                             merchant       category     amt      first  \\\n",
       "0          fraud_Rippin, Kub and Mann       misc_net    4.97   Jennifer   \n",
       "1     fraud_Heller, Gutmann and Zieme    grocery_pos  107.23  Stephanie   \n",
       "2                fraud_Lind-Buckridge  entertainment  220.11     Edward   \n",
       "3  fraud_Kutch, Hermiston and Farrell  gas_transport   45.00     Jeremy   \n",
       "4                 fraud_Keeling-Crist       misc_pos   41.96      Tyler   \n",
       "\n",
       "      last gender                        street  ...      lat      long  \\\n",
       "0    Banks      F                561 Perry Cove  ...  36.0788  -81.1781   \n",
       "1     Gill      F  43039 Riley Greens Suite 393  ...  48.8878 -118.2105   \n",
       "2  Sanchez      M      594 White Dale Suite 530  ...  42.1808 -112.2620   \n",
       "3    White      M   9443 Cynthia Court Apt. 038  ...  46.2306 -112.1138   \n",
       "4   Garcia      M              408 Bradley Rest  ...  38.4207  -79.4629   \n",
       "\n",
       "   city_pop                                job         dob  \\\n",
       "0      3495          Psychologist, counselling  1988-03-09   \n",
       "1       149  Special educational needs teacher  1978-06-21   \n",
       "2      4154        Nature conservation officer  1962-01-19   \n",
       "3      1939                    Patent attorney  1967-01-12   \n",
       "4        99     Dance movement psychotherapist  1986-03-28   \n",
       "\n",
       "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
       "0  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315   \n",
       "1  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462   \n",
       "2  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481   \n",
       "3  6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071   \n",
       "4  a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459   \n",
       "\n",
       "   is_fraud  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load fraud dataset\n",
    "df = pd.read_csv(r'/Users/shiva/PycharmProjects/mlearn_poc/dailyprojects/project8/fraudTrain.csv')  # update path if needed\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"Memory usage before optimization:\")\n",
    "df.info(memory_usage='deep')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc106e8",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ DataFrame Memory Optimization\n",
    "**Purpose:** Downcast numeric types to reduce memory footprint.\n",
    "**Why:** Smaller memory allows faster operations and avoids swapping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a101a",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 3\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0574052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1296675 entries, 0 to 1296674\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count    Dtype  \n",
      "---  ------                 --------------    -----  \n",
      " 0   Unnamed: 0             1296675 non-null  int32  \n",
      " 1   trans_date_trans_time  1296675 non-null  object \n",
      " 2   cc_num                 1296675 non-null  int32  \n",
      " 3   merchant               1296675 non-null  object \n",
      " 4   category               1296675 non-null  object \n",
      " 5   amt                    1296675 non-null  float32\n",
      " 6   first                  1296675 non-null  object \n",
      " 7   last                   1296675 non-null  object \n",
      " 8   gender                 1296675 non-null  object \n",
      " 9   street                 1296675 non-null  object \n",
      " 10  city                   1296675 non-null  object \n",
      " 11  state                  1296675 non-null  object \n",
      " 12  zip                    1296675 non-null  int32  \n",
      " 13  lat                    1296675 non-null  float32\n",
      " 14  long                   1296675 non-null  float32\n",
      " 15  city_pop               1296675 non-null  int32  \n",
      " 16  job                    1296675 non-null  object \n",
      " 17  dob                    1296675 non-null  object \n",
      " 18  trans_num              1296675 non-null  object \n",
      " 19  unix_time              1296675 non-null  int32  \n",
      " 20  merch_lat              1296675 non-null  float32\n",
      " 21  merch_long             1296675 non-null  float32\n",
      " 22  is_fraud               1296675 non-null  int32  \n",
      "dtypes: float32(5), int32(6), object(12)\n",
      "memory usage: 980.6 MB\n"
     ]
    }
   ],
   "source": [
    "# Downcast floats to float32 and ints to int32\n",
    "float_cols = df.select_dtypes(include=['float64']).columns\n",
    "int_cols = df.select_dtypes(include=['int64']).columns\n",
    "\n",
    "df[float_cols] = df[float_cols].astype('float32')\n",
    "df[int_cols]   = df[int_cols].astype('int32')\n",
    "\n",
    "print(\"Memory usage after optimization:\")\n",
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5971350c",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Parse Transaction Timestamp & Derive Features\n",
    "**Purpose:** Convert string timestamp to datetime, extract hour/day features.\n",
    "**Why:** Time-of-day and weekday patterns can reveal fraud behaviors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785e267",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 4\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c9f2c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trans_ts</th>\n",
       "      <th>hour</th>\n",
       "      <th>dow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 00:00:18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 00:00:44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 00:00:51</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 00:01:16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 00:03:06</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             trans_ts  hour  dow\n",
       "0 2019-01-01 00:00:18     0    1\n",
       "1 2019-01-01 00:00:44     0    1\n",
       "2 2019-01-01 00:00:51     0    1\n",
       "3 2019-01-01 00:01:16     0    1\n",
       "4 2019-01-01 00:03:06     0    1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse timestamp\n",
    "df['trans_ts'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "# Drop redundant columns\n",
    "df.drop(columns=['trans_date_trans_time', 'unix_time'], inplace=True)\n",
    "\n",
    "# Derive features\n",
    "df['hour'] = df['trans_ts'].dt.hour\n",
    "df['dow']  = df['trans_ts'].dt.dayofweek\n",
    "\n",
    "df[['trans_ts', 'hour', 'dow']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ccbecb",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Drop Unnecessary ID/PII Columns\n",
    "**Purpose:** Remove columns that leak information or have no predictive value.\n",
    "**Why:** IDs and PII add noise or risk leakage but rarely generalize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba49216",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 5\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ae29bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['merchant', 'category', 'amt', 'gender', 'lat', 'long', 'city_pop',\n",
       "       'job', 'dob', 'merch_lat', 'merch_long', 'is_fraud', 'trans_ts', 'hour',\n",
       "       'dow'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop card number, transaction ID, personal identifiers\n",
    "df.drop(columns=['Unnamed: 0','cc_num','trans_num','first','last','street','city','state','zip'], inplace=True)\n",
    "\n",
    "# Preview remaining columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ae2c37",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Target Distribution (Class Imbalance)\n",
    "**Purpose:** Check fraud ratio to guide evaluation and sampling strategies.\n",
    "**Why:** Imbalanced classes require specialized metrics and handling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8403f94",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 6\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26f7732c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud cases fraction: 0.005789 (0.579% )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "is_fraud\n",
       "0    1289169\n",
       "1       7506\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fraction of fraud cases\n",
    "fraud_ratio = df['is_fraud'].mean()\n",
    "print(f\"Fraud cases fraction: {fraud_ratio:.6f} ({fraud_ratio*100:.3f}% )\")\n",
    "df['is_fraud'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863b4ce",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Data Types Overview\n",
    "**Purpose:** Verify each column's type to plan preprocessing.\n",
    "**Next:** Convert object types to appropriate categories or numerics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe899dfd",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 7\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04db7e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object            5\n",
       "float32           5\n",
       "int32             4\n",
       "datetime64[ns]    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215312a",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Missing Value Summary\n",
    "**Purpose:** Identify missing data proportions.\n",
    "**Next:** Plan imputation or column removal if too many NaNs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7240d9",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 8\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d279ace5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "missing_pct = df.isna().mean().sort_values(ascending=False)\n",
    "print(missing_pct[missing_pct>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d7ef2",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Categorical Cardinality\n",
    "**Purpose:** Find high-cardinality columns to choose encoding strategy.\n",
    "**Next:** Low-card cols: one-hot; high-card: frequency/target encode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d955f800",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 9\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e73603d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dob         968\n",
       "merchant    693\n",
       "job         494\n",
       "category     14\n",
       "gender        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "df[cat_cols].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bfc691",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Basic Statistics & Outlier Detection\n",
    "**Purpose:** Summarize distributions, spot anomalies.\n",
    "**Next:** Winsorize or log-transform extreme outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8faf1",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 10\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5110274f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>merchant</th>\n",
       "      <td>1296675</td>\n",
       "      <td>693</td>\n",
       "      <td>fraud_Kilback LLC</td>\n",
       "      <td>4403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>1296675</td>\n",
       "      <td>14</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>131659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amt</th>\n",
       "      <td>1296675.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.351028</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.65</td>\n",
       "      <td>47.52</td>\n",
       "      <td>83.139999</td>\n",
       "      <td>28948.900391</td>\n",
       "      <td>160.31604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>1296675</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>709863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lat</th>\n",
       "      <td>1296675.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.537624</td>\n",
       "      <td>20.0271</td>\n",
       "      <td>34.620499</td>\n",
       "      <td>39.354301</td>\n",
       "      <td>41.940399</td>\n",
       "      <td>66.693298</td>\n",
       "      <td>5.075809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>1296675.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-90.226357</td>\n",
       "      <td>-165.672302</td>\n",
       "      <td>-96.797997</td>\n",
       "      <td>-87.476898</td>\n",
       "      <td>-80.157997</td>\n",
       "      <td>-67.950302</td>\n",
       "      <td>13.759077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city_pop</th>\n",
       "      <td>1296675.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88824.440563</td>\n",
       "      <td>23.0</td>\n",
       "      <td>743.0</td>\n",
       "      <td>2456.0</td>\n",
       "      <td>20328.0</td>\n",
       "      <td>2906700.0</td>\n",
       "      <td>301956.360689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job</th>\n",
       "      <td>1296675</td>\n",
       "      <td>494</td>\n",
       "      <td>Film/video editor</td>\n",
       "      <td>9779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dob</th>\n",
       "      <td>1296675</td>\n",
       "      <td>968</td>\n",
       "      <td>1977-03-23</td>\n",
       "      <td>5636</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merch_lat</th>\n",
       "      <td>1296675.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.537346</td>\n",
       "      <td>19.027784</td>\n",
       "      <td>34.733572</td>\n",
       "      <td>39.365681</td>\n",
       "      <td>41.957163</td>\n",
       "      <td>67.510269</td>\n",
       "      <td>5.109788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merch_long</th>\n",
       "      <td>1296675.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-90.226448</td>\n",
       "      <td>-166.671249</td>\n",
       "      <td>-96.897278</td>\n",
       "      <td>-87.438393</td>\n",
       "      <td>-80.236794</td>\n",
       "      <td>-66.950905</td>\n",
       "      <td>13.771091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_fraud</th>\n",
       "      <td>1296675.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans_ts</th>\n",
       "      <td>1296675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-10-03 12:47:28.070214144</td>\n",
       "      <td>2019-01-01 00:00:18</td>\n",
       "      <td>2019-06-03 19:12:22.500000</td>\n",
       "      <td>2019-10-03 07:35:47</td>\n",
       "      <td>2020-01-28 15:02:55.500000</td>\n",
       "      <td>2020-06-21 12:13:37</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>1296675.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.804858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>6.817824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dow</th>\n",
       "      <td>1296675.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.070604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.198153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                count unique                top    freq  \\\n",
       "merchant      1296675    693  fraud_Kilback LLC    4403   \n",
       "category      1296675     14      gas_transport  131659   \n",
       "amt         1296675.0    NaN                NaN     NaN   \n",
       "gender        1296675      2                  F  709863   \n",
       "lat         1296675.0    NaN                NaN     NaN   \n",
       "long        1296675.0    NaN                NaN     NaN   \n",
       "city_pop    1296675.0    NaN                NaN     NaN   \n",
       "job           1296675    494  Film/video editor    9779   \n",
       "dob           1296675    968         1977-03-23    5636   \n",
       "merch_lat   1296675.0    NaN                NaN     NaN   \n",
       "merch_long  1296675.0    NaN                NaN     NaN   \n",
       "is_fraud    1296675.0    NaN                NaN     NaN   \n",
       "trans_ts      1296675    NaN                NaN     NaN   \n",
       "hour        1296675.0    NaN                NaN     NaN   \n",
       "dow         1296675.0    NaN                NaN     NaN   \n",
       "\n",
       "                                     mean                  min  \\\n",
       "merchant                              NaN                  NaN   \n",
       "category                              NaN                  NaN   \n",
       "amt                             70.351028                  1.0   \n",
       "gender                                NaN                  NaN   \n",
       "lat                             38.537624              20.0271   \n",
       "long                           -90.226357          -165.672302   \n",
       "city_pop                     88824.440563                 23.0   \n",
       "job                                   NaN                  NaN   \n",
       "dob                                   NaN                  NaN   \n",
       "merch_lat                       38.537346            19.027784   \n",
       "merch_long                     -90.226448          -166.671249   \n",
       "is_fraud                         0.005789                  0.0   \n",
       "trans_ts    2019-10-03 12:47:28.070214144  2019-01-01 00:00:18   \n",
       "hour                            12.804858                  0.0   \n",
       "dow                              3.070604                  0.0   \n",
       "\n",
       "                                   25%                  50%  \\\n",
       "merchant                           NaN                  NaN   \n",
       "category                           NaN                  NaN   \n",
       "amt                               9.65                47.52   \n",
       "gender                             NaN                  NaN   \n",
       "lat                          34.620499            39.354301   \n",
       "long                        -96.797997           -87.476898   \n",
       "city_pop                         743.0               2456.0   \n",
       "job                                NaN                  NaN   \n",
       "dob                                NaN                  NaN   \n",
       "merch_lat                    34.733572            39.365681   \n",
       "merch_long                  -96.897278           -87.438393   \n",
       "is_fraud                           0.0                  0.0   \n",
       "trans_ts    2019-06-03 19:12:22.500000  2019-10-03 07:35:47   \n",
       "hour                               7.0                 14.0   \n",
       "dow                                1.0                  3.0   \n",
       "\n",
       "                                   75%                  max            std  \n",
       "merchant                           NaN                  NaN            NaN  \n",
       "category                           NaN                  NaN            NaN  \n",
       "amt                          83.139999         28948.900391      160.31604  \n",
       "gender                             NaN                  NaN            NaN  \n",
       "lat                          41.940399            66.693298       5.075809  \n",
       "long                        -80.157997           -67.950302      13.759077  \n",
       "city_pop                       20328.0            2906700.0  301956.360689  \n",
       "job                                NaN                  NaN            NaN  \n",
       "dob                                NaN                  NaN            NaN  \n",
       "merch_lat                    41.957163            67.510269       5.109788  \n",
       "merch_long                  -80.236794           -66.950905      13.771091  \n",
       "is_fraud                           0.0                  1.0       0.075863  \n",
       "trans_ts    2020-01-28 15:02:55.500000  2020-06-21 12:13:37            NaN  \n",
       "hour                              19.0                 23.0       6.817824  \n",
       "dow                                5.0                  6.0       2.198153  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc0342e",
   "metadata": {},
   "source": [
    "## üîü Detailed Class Imbalance Analysis\n",
    "**Purpose:** Evaluate false negative vs false positive costs.\n",
    "**Next:** Plan SMOTE or class weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3cfdeb",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 11\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbd4bf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            count     ratio\n",
      "is_fraud                   \n",
      "0         1289169  0.994211\n",
      "1            7506  0.005789\n"
     ]
    }
   ],
   "source": [
    "# Class counts & ratios\n",
    "counts = df['is_fraud'].value_counts()\n",
    "ratios = df['is_fraud'].value_counts(normalize=True)\n",
    "print(pd.concat([counts, ratios], axis=1, keys=['count','ratio']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd42ec",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Leakage Checks\n",
    "**Purpose:** Detect features that perfectly predict the target or leak future info.\n",
    "**Next:** Drop or re-engineer leaking features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b50794",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 12\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa530921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top numeric correlations:\n",
      "is_fraud      1.000000\n",
      "amt           0.219404\n",
      "hour          0.013799\n",
      "city_pop      0.002136\n",
      "lat           0.001894\n",
      "merch_lat     0.001741\n",
      "dow           0.001739\n",
      "merch_long    0.001721\n",
      "long          0.001721\n",
      "Name: is_fraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# # Correlation with target for numeric features\n",
    "# num_corr = df.corr()['is_fraud'].abs().sort_values(ascending=False)\n",
    "# print(\"Top numeric correlations:\")\n",
    "# print(num_corr.head(10))\n",
    "\n",
    "# # Categorical perfect predictor check\n",
    "# for col in cat_cols:\n",
    "#     if df.groupby(col)['is_fraud'].nunique().eq(1).all():\n",
    "#         print(f\"Potential leakage in {col}\")\n",
    "        \n",
    "# ‚úÖ Only use numeric columns for correlation\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "num_corr = df[numeric_cols].corr()['is_fraud'].abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top numeric correlations:\")\n",
    "print(num_corr.head(10))\n",
    "\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in cat_cols:\n",
    "    try:\n",
    "        if df.groupby(col)['is_fraud'].nunique().eq(1).all():\n",
    "            print(f\"‚ö†Ô∏è Potential leakage in '{col}' ‚Äî perfect predictor\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not evaluate column {col}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# # Correlation with target for numeric features\n",
    "# num_corr = df.corr()['is_fraud'].abs().sort_values(ascending=False)\n",
    "# print(\"Top numeric correlations:\")\n",
    "# print(num_corr.head(10))\n",
    "\n",
    "# # Categorical perfect predictor check\n",
    "# for col in cat_cols:\n",
    "#     if df.groupby(col)['is_fraud'].nunique().eq(1).all():\n",
    "#         print(f\"Potential leakage in {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fdc17f",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Duplicate Rows\n",
    "**Purpose:** Remove exact duplicates that can skew training.\n",
    "**Next:** Drop duplicates if found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842d0e7a",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 13\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21652320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "dup_count = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {dup_count}\")\n",
    "if dup_count > 0:\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(\"Duplicates dropped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662eb126",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Time Order & Concept Drift\n",
    "**Purpose:** Ensure chronological order and detect drift between early vs late transactions.\n",
    "**Next:** Use time-based CV or retrain triggers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5dd6c",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 14\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5010696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by transaction time\n",
    "df.sort_values('trans_ts', inplace=True)\n",
    "\n",
    "# Train-test split by time\n",
    "split_date = df['trans_ts'].quantile(0.8)\n",
    "train = df[df['trans_ts'] < split_date]\n",
    "test  = df[df['trans_ts'] >= split_date]\n",
    "\n",
    "# Compute PSI for 'amt_log' if exists\n",
    "if 'amt_log' in df.columns:\n",
    "    import numpy as np\n",
    "    def psi(expected, actual, bins=10):\n",
    "        def _bin(x, edges): return np.digitize(x, edges[:-1])\n",
    "        edges = np.histogram(expected, bins=bins)[1]\n",
    "        e_perc = np.bincount(_bin(expected, edges), minlength=bins) / len(expected)\n",
    "        a_perc = np.bincount(_bin(actual, edges), minlength=bins) / len(actual)\n",
    "        return np.sum((a_perc - e_perc) * np.log((a_perc + 1e-6)/(e_perc + 1e-6)))\n",
    "    psi_val = psi(train['amt_log'], test['amt_log'])\n",
    "    print(f\"PSI for amt_log: {psi_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f2670c",
   "metadata": {},
   "source": [
    "## üíæ Save Cleaned Data\n",
    "**Purpose:** Persist the cleaned and enriched dataset for modeling steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e02b9",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 15\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6831511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to 'cc_fraud_cleaned.parquet'\n"
     ]
    }
   ],
   "source": [
    "df.to_parquet(r'/Users/shiva/PycharmProjects/mlearn_poc/dailyprojects/project8/cc_fraud_cleaned.parquet', index=False)\n",
    "print(\"Cleaned data saved to 'cc_fraud_cleaned.parquet'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d550e484",
   "metadata": {},
   "source": [
    "## üåü Feature Engineering Steps\n",
    "**Purpose:** Transform raw cleaned data into model-ready features.\n",
    "Each section below creates or encodes features for the fraud detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371e44c",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Handle Missing Values\n",
    "**What to do:** Impute or drop missing data.\n",
    "**Code:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f79e3",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 16\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d23e362a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values if present (example: none expected)\n",
    "# Numeric: median, Categorical: mode or 'Unknown'\n",
    "for col in df.columns:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        if df[col].dtype in ['float32','float64','int32','int64']:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "        else:\n",
    "            df[col].fillna('Unknown', inplace=True)\n",
    "print(\"Missing values after imputation:\", df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c956e",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Encoding Categorical Variables\n",
    "**What to do:** Convert categories into numeric representations.\n",
    "**Code:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0929dc",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 17\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a1f78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Binary encoding for gender\n",
    "# df['gender_enc'] = LabelEncoder().fit_transform(df['gender'])\n",
    "\n",
    "# # One-hot for low-cardinality 'category'\n",
    "# df = pd.concat([df, pd.get_dummies(df['category'], prefix='cat', drop_first=True)], axis=1)\n",
    "# df.drop(columns=['gender','category'], inplace=True)\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# df['gender_enc'] = LabelEncoder().fit_transform(df['gender'])\n",
    "# df = pd.concat([df, pd.get_dummies(df['category'], drop_first=True, prefix='cat')], axis=1)\n",
    "\n",
    "# df.drop(columns=['gender', 'category'], inplace=True)\n",
    "\n",
    "\n",
    "# # Frequency encoding for 'merchant' and 'job'\n",
    "# # for col in ['merchant','job']:\n",
    "# #     freq = df[col].value_counts(normalize=True)\n",
    "# #     df[f'{col}_freq'] = df[col].map(freq)\n",
    "# #     df.drop(columns=[col], inplace=True)\n",
    "# # df.head()\n",
    "\n",
    "# for col in ['merchant', 'job']:\n",
    "#     freq = df[col].value_counts(normalize=True)\n",
    "#     df[f'{col}_freq'] = df[col].map(freq)\n",
    "\n",
    "# df.drop(columns=['merchant', 'job'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. Encode high-cardinality string features\n",
    "for col in ['merchant', 'job']:\n",
    "    if col in df.columns:\n",
    "        freq_map = df[col].value_counts(normalize=True)\n",
    "        df[f'{col}_freq'] = df[col].map(freq_map)\n",
    "        df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# 2. Encode low-cardinality features\n",
    "if 'gender' in df.columns:\n",
    "    df['gender_enc'] = LabelEncoder().fit_transform(df['gender'])\n",
    "    df.drop(columns=['gender'], inplace=True)\n",
    "\n",
    "if 'category' in df.columns:\n",
    "    df = pd.concat([df, pd.get_dummies(df['category'], prefix='cat', drop_first=True)], axis=1)\n",
    "    df.drop(columns=['category'], inplace=True)\n",
    "\n",
    "# 3. Drop datetime columns unless feature engineered\n",
    "df.drop(columns=['dob', 'trans_ts'], inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d884aa4",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Log Transform Skewed Numeric Features\n",
    "**What to do:** Reduce skew in monetary features.\n",
    "**Code:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7edf6",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 18\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d2f48e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness amt_log: -0.2988528\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Log transform amount\n",
    "df['amt_log'] = np.log1p(df['amt'])\n",
    "df.drop(columns=['amt'], inplace=True)\n",
    "\n",
    "# Check skew\n",
    "print(\"Skewness amt_log:\", df['amt_log'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e30e8f",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ Interaction & Polynomial Features\n",
    "**What to do:** Capture nonlinear relationships.\n",
    "**Code:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26aacb",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 19\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "293c7ce3-c524-4416-98bb-6f211a674249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def haversine_vectorized(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "# Compute distance column\n",
    "df['dist_km'] = haversine_vectorized(df['lat'], df['long'], df['merch_lat'], df['merch_long'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b55dd79-53b8-40a0-b6b5-f5e8926e2763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lat', 'long', 'city_pop', 'merch_lat', 'merch_long', 'is_fraud',\n",
       "       'hour', 'dow', 'merchant_freq', 'job_freq', 'gender_enc',\n",
       "       'cat_food_dining', 'cat_gas_transport', 'cat_grocery_net',\n",
       "       'cat_grocery_pos', 'cat_health_fitness', 'cat_home', 'cat_kids_pets',\n",
       "       'cat_misc_net', 'cat_misc_pos', 'cat_personal_care', 'cat_shopping_net',\n",
       "       'cat_shopping_pos', 'cat_travel', 'amt_log', 'dist_km'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a65e074c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dob'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'dob'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Convert dob to datetime if not already\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mdob\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdob\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Calculate age at the time of transaction\u001b[39;00m\n\u001b[32m      5\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mage\u001b[39m\u001b[33m'\u001b[39m] = (df[\u001b[33m'\u001b[39m\u001b[33mtrans_ts\u001b[39m\u001b[33m'\u001b[39m] - df[\u001b[33m'\u001b[39m\u001b[33mdob\u001b[39m\u001b[33m'\u001b[39m]).dt.days // \u001b[32m365\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'dob'"
     ]
    }
   ],
   "source": [
    "# Convert dob to datetime if not already\n",
    "df['dob'] = pd.to_datetime(df['dob'])\n",
    "\n",
    "# Calculate age at the time of transaction\n",
    "df['age'] = (df['trans_ts'] - df['dob']).dt.days // 365\n",
    "\n",
    "# Interaction: amount x distance\n",
    "df['amt_dist'] = df['amt_log'] * df['dist_km']\n",
    "\n",
    "# Polynomial: squared terms\n",
    "for col in ['amt_log', 'dist_km', 'age']:\n",
    "    df[f'{col}_sq'] = df[col] ** 2\n",
    "\n",
    "df[['amt_dist','amt_log_sq','dist_km_sq','age_sq']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeb97fa",
   "metadata": {},
   "source": [
    "### 5Ô∏è‚É£ Feature Scaling\n",
    "**What to do:** Standardize features for linear models or distance-based methods.\n",
    "**Code:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c3295",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 20\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scale_cols = ['amt_log','dist_km','age','hour','dow','amt_dist','amt_log_sq','dist_km_sq','age_sq']\n",
    "df[scale_cols] = scaler.fit_transform(df[scale_cols])\n",
    "\n",
    "df[scale_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ceeb64",
   "metadata": {},
   "source": [
    "## ‚úÖ Final Feature Matrix\n",
    "**Purpose:** Review final columns and prepare X, y for modeling.\n",
    "**Code:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921be7a9",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Cell 21\n",
    "**What I'm checking / Why**:  \n",
    "- *Goal*: Describe the primary purpose of this cell (e.g., load data, cast dtypes).  \n",
    "- *Expectation*: What a 'healthy' output looks like (shape, memory, etc.).  \n",
    "\n",
    "**What I look for in the output**:  \n",
    "- Key flags (null counts, skew, imbalance, leakage signals).  \n",
    "\n",
    "**Typical next actions**:  \n",
    "- If results are normal ‚Üí proceed.  \n",
    "- If anomalies appear ‚Üí how I'd fix (impute, drop, transform).  \n",
    "\n",
    "**Pros / Cons & Alternatives**:  \n",
    "- Why I chose this method vs alternatives.  \n",
    "- Potential drawbacks or edge‚Äëcases.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10b240ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat              float32\n",
      "long             float32\n",
      "city_pop           int32\n",
      "merch_lat        float32\n",
      "merch_long       float32\n",
      "hour               int32\n",
      "dow                int32\n",
      "merchant_freq    float64\n",
      "job_freq         float64\n",
      "gender_enc         int64\n",
      "amt_log          float32\n",
      "dist_km          float32\n",
      "dtype: object\n",
      "(1296675, 12)\n",
      "Final feature matrix shape: (1296675, 12)\n",
      "Features: ['lat', 'long', 'city_pop', 'merch_lat', 'merch_long', 'hour', 'dow', 'merchant_freq', 'job_freq', 'gender_enc', 'amt_log', 'dist_km']\n"
     ]
    }
   ],
   "source": [
    "# Drop any remaining raw columns if needed\n",
    "# Prepare X and y\n",
    "import numpy as np\n",
    "# Keep only numeric columns\n",
    "X = df.select_dtypes(include=['number']).drop(columns=['is_fraud'])\n",
    "\n",
    "X.select_dtypes(exclude=['number']).columns\n",
    "# Fill missing or problematic values\n",
    "X = X.fillna(0)\n",
    "X = X.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Confirm shape and types\n",
    "print(X.dtypes)\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "\n",
    "# X = df.drop(columns=['trans_ts','is_fraud','lat','long','merch_lat','merch_long','dob'])\n",
    "# y = df['is_fraud']\n",
    "print(\"Final feature matrix shape:\", X.shape)\n",
    "print(\"Features:\", X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf345e9",
   "metadata": {},
   "source": [
    "# üöÄ Model Development & Evaluation\n",
    "This section builds, tunes, and evaluates a fraud detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fedc31",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Model Cell 1\n",
    "**Purpose**: Load the cleaned feature set saved previously.\n",
    "**Expectation**: X should be feature matrix, y target.\n",
    "**Next**: Split into train/test.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b1ad241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1037340, 14) (259335, 14) 0.00578884454470087\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_parquet(r'/Users/shiva/PycharmProjects/mlearn_poc/dailyprojects/project8/cc_fraud_cleaned.parquet')\n",
    "X = df.drop(columns=['is_fraud'])\n",
    "y = df['is_fraud']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e97c7",
   "metadata": {},
   "source": [
    "### üß† Thought Process for Model Cell 2\n",
    "**Purpose**: Construct pipeline with SMOTE (over-sampling) and RandomForest with class_weight.\n",
    "**Why**: Handle extreme imbalance and leverage tree robustness.\n",
    "**Alternatives**: Use XGBoost; try undersampling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c757cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('rf', RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35935f68",
   "metadata": {},
   "source": [
    "### ‚úÖ Data Cleanup: Encode non-numeric columns and remove datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "301731d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode high-cardinality string features using frequency\n",
    "for col in ['merchant', 'job']:\n",
    "    if col in df.columns:\n",
    "        freq_map = df[col].value_counts(normalize=True)\n",
    "        df[f'{col}_freq'] = df[col].map(freq_map)\n",
    "        df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# Encode low-cardinality features\n",
    "if 'gender' in df.columns:\n",
    "    df['gender_enc'] = LabelEncoder().fit_transform(df['gender'])\n",
    "    df.drop(columns=['gender'], inplace=True)\n",
    "\n",
    "if 'category' in df.columns:\n",
    "    df = pd.concat([df, pd.get_dummies(df['category'], prefix='cat', drop_first=True)], axis=1)\n",
    "    df.drop(columns=['category'], inplace=True)\n",
    "\n",
    "# Drop datetime columns\n",
    "df.drop(columns=['dob', 'trans_ts'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da4406f",
   "metadata": {},
   "source": [
    "### üßÆ Prepare Final Numeric Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb779e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1037340, 11), (259335, 11), np.float64(0.00578884454470087))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.select_dtypes(include=['number']).drop(columns=['is_fraud'])\n",
    "y = df['is_fraud']\n",
    "\n",
    "# Final check\n",
    "assert X.select_dtypes(exclude=['number']).shape[1] == 0\n",
    "assert X.isna().sum().sum() == 0\n",
    "assert y.nunique() == 2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3f550",
   "metadata": {},
   "source": [
    "### üöÄ Model Pipeline with SMOTE + RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "422824bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('rf', RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb1b27e",
   "metadata": {},
   "source": [
    "### üîç Hyperparameter Tuning using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02b67dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_dist = {\n",
    "    'rf__n_estimators': randint(300, 1000),\n",
    "    'rf__max_depth': randint(5, 20),\n",
    "    'rf__min_samples_split': randint(2, 10),\n",
    "    'rf__min_samples_leaf': randint(1, 5),\n",
    "    'rf__max_features': uniform(0.2, 0.6)\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    scoring='average_precision',\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    error_score='raise'  # for debugging\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538060f8",
   "metadata": {},
   "source": [
    "### üîÅ Fit the Model (may take a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c7c93d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest PR-AUC:\u001b[39m\u001b[33m\"\u001b[39m, search.best_score_)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest params:\u001b[39m\u001b[33m\"\u001b[39m, search.best_params_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1951\u001b[39m, in \u001b[36mRandomizedSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1950\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1951\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/sklearn/model_selection/_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/joblib/parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/joblib/parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/mlearn_poc/.venv/lib/python3.13/site-packages/joblib/parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "search.fit(X_train, y_train)\n",
    "print(\"Best PR-AUC:\", search.best_score_)\n",
    "print(\"Best params:\", search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a24ba81",
   "metadata": {},
   "source": [
    "### üìä Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6380ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"Test PR-AUC:\", average_precision_score(y_test, y_proba))\n",
    "print(\"Test ROC-AUC:\", roc_auc_score(y_test, y_proba))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
