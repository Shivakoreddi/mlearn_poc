{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889a7115",
   "metadata": {},
   "source": [
    "# ðŸ”§ Reusable Hyperparameterâ€‘Tuning Workbook\n",
    "Generated 2025â€‘04â€‘19 20:44"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716345d",
   "metadata": {},
   "source": [
    "Fill the **CONFIG** cell, then run cells topâ€‘toâ€‘bottom. Works for tabular datasets in CSV or a synthetic demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== CONFIG =====================\n",
    "DATA_PATH      = None             # e.g. 'data/credit.csv'\n",
    "TARGET_COL     = 'target'         # label column name\n",
    "PROBLEM_TYPE   = 'classification' # 'classification' or 'regression'\n",
    "BASE_MODEL     = 'RandomForest'   # 'RandomForest'|'GradientBoosting'|'Logistic'|'Ridge'\n",
    "METRIC         = 'f1'             # scoring metric for CV\n",
    "N_SPLITS       = 5                # CV folds\n",
    "SEARCH_STRATEGY= 'random'         # 'grid' or 'random'\n",
    "N_ITER         = 50               # random-search iterations\n",
    "# ==================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aeb7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, warnings, time, json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import (classification_report, roc_auc_score, f1_score, accuracy_score, r2_score, mean_squared_error)\n",
    "from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor,\n",
    "                              GradientBoostingClassifier, GradientBoostingRegressor)\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a998f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_PATH and Path(DATA_PATH).exists():\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f'Loaded {df.shape} from {DATA_PATH}')\n",
    "else:\n",
    "    print('No DATA_PATH provided; generating synthetic dataset')\n",
    "    if PROBLEM_TYPE=='classification':\n",
    "        from sklearn.datasets import make_classification\n",
    "        X_syn, y_syn = make_classification(n_samples=1000, n_features=20, n_informative=10, random_state=42)\n",
    "    else:\n",
    "        from sklearn.datasets import make_regression\n",
    "        X_syn, y_syn = make_regression(n_samples=1000, n_features=20, noise=0.4, random_state=42)\n",
    "    df = pd.DataFrame(X_syn, columns=[f'feat_{i}' for i in range(X_syn.shape[1])])\n",
    "    df[TARGET_COL]=y_syn\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27eed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape:', df.shape)\n",
    "print('Target distribution / stats:')\n",
    "print(df[TARGET_COL].value_counts(normalize=True) if PROBLEM_TYPE=='classification' else df[TARGET_COL].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns=[TARGET_COL]), df[TARGET_COL]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y if PROBLEM_TYPE=='classification' else None, random_state=42)\n",
    "print('Train:', X_train.shape, 'Test:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_space(model_name):\n",
    "    if PROBLEM_TYPE=='classification':\n",
    "        if model_name=='RandomForest':\n",
    "            model=RandomForestClassifier(random_state=42)\n",
    "            grid={'n_estimators':[100,300,600],\n",
    "                  'max_depth':[None,5,10,20],\n",
    "                  'min_samples_split':[2,5,10],\n",
    "                  'min_samples_leaf':[1,2,4],\n",
    "                  'max_features':['sqrt','log2',0.5]}\n",
    "            dist={'n_estimators':randint(200,1000),\n",
    "                  'max_depth':randint(3,25),\n",
    "                  'min_samples_split':randint(2,15),\n",
    "                  'min_samples_leaf':randint(1,10),\n",
    "                  'max_features':uniform(0.2,0.8)}\n",
    "        elif model_name=='GradientBoosting':\n",
    "            model=GradientBoostingClassifier(random_state=42)\n",
    "            grid={'n_estimators':[100,300,500],\n",
    "                  'learning_rate':[0.01,0.05,0.1],\n",
    "                  'max_depth':[2,3,4],\n",
    "                  'subsample':[0.6,0.8,1.0]}\n",
    "            dist={'n_estimators':randint(100,600),\n",
    "                  'learning_rate':loguniform(1e-3,0.2),\n",
    "                  'max_depth':randint(2,6),\n",
    "                  'subsample':uniform(0.5,0.5)}\n",
    "        elif model_name=='Logistic':\n",
    "            model=LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "            grid={'C':[0.01,0.1,1,10],'penalty':['l1','l2']}\n",
    "            dist={'C':loguniform(1e-3,10),'penalty':['l1','l2']}\n",
    "        else:\n",
    "            raise ValueError('Unsupported model')\n",
    "    else:\n",
    "        if model_name=='RandomForest':\n",
    "            model=RandomForestRegressor(random_state=42)\n",
    "            grid={'n_estimators':[200,400,800],\n",
    "                  'max_depth':[None,5,10,20],\n",
    "                  'min_samples_split':[2,5,10],\n",
    "                  'min_samples_leaf':[1,2,4],\n",
    "                  'max_features':['sqrt','log2',0.6]}\n",
    "            dist={'n_estimators':randint(200,1200),\n",
    "                  'max_depth':randint(3,30),\n",
    "                  'min_samples_split':randint(2,15),\n",
    "                  'min_samples_leaf':randint(1,10),\n",
    "                  'max_features':uniform(0.3,0.7)}\n",
    "        elif model_name=='GradientBoosting':\n",
    "            model=GradientBoostingRegressor(random_state=42)\n",
    "            grid={'n_estimators':[100,300,500],\n",
    "                  'learning_rate':[0.01,0.05,0.1],\n",
    "                  'max_depth':[2,3,4],\n",
    "                  'subsample':[0.6,0.8,1.0]}\n",
    "            dist={'n_estimators':randint(100,600),\n",
    "                  'learning_rate':loguniform(1e-3,0.2),\n",
    "                  'max_depth':randint(2,6),\n",
    "                  'subsample':uniform(0.5,0.5)}\n",
    "        elif model_name=='Ridge':\n",
    "            model=Ridge()\n",
    "            grid={'alpha':[0.1,1,10,50]}\n",
    "            dist={'alpha':loguniform(1e-3,100)}\n",
    "        else:\n",
    "            raise ValueError('Unsupported model')\n",
    "    return model, grid, dist\n",
    "\n",
    "model, grid_params, dist_params = get_model_and_space(BASE_MODEL)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nðŸ”¹ Baseline training...')\n",
    "model.fit(X_train, y_train)\n",
    "if PROBLEM_TYPE=='classification':\n",
    "    pred=model.predict(X_test)\n",
    "    baseline=f1_score(y_test,pred) if METRIC=='f1' else accuracy_score(y_test,pred)\n",
    "    print(f'Baseline {METRIC}:', baseline)\n",
    "else:\n",
    "    pred=model.predict(X_test)\n",
    "    baseline=r2_score(y_test,pred)\n",
    "    print('Baseline R2:', baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199070c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42) if PROBLEM_TYPE=='classification'      else KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e05e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SEARCH_STRATEGY=='grid':\n",
    "    searcher=GridSearchCV(model, grid_params, scoring=METRIC, cv=cv, n_jobs=-1, verbose=1)\n",
    "else:\n",
    "    searcher=RandomizedSearchCV(model, dist_params, n_iter=N_ITER, scoring=METRIC,\n",
    "                                cv=cv, n_jobs=-1, random_state=42, verbose=1)\n",
    "searcher.fit(X_train, y_train)\n",
    "print('Best params:', searcher.best_params_)\n",
    "print('Best CV score:', searcher.best_score_)\n",
    "best_model=searcher.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa392c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROBLEM_TYPE=='classification':\n",
    "    y_pred=best_model.predict(X_test)\n",
    "    if hasattr(best_model,'predict_proba'):\n",
    "        y_prob=best_model.predict_proba(X_test)[:,1]\n",
    "        print('Test ROCâ€‘AUC:', roc_auc_score(y_test,y_prob))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "else:\n",
    "    y_pred=best_model.predict(X_test)\n",
    "    print('Test R2:', r2_score(y_test,y_pred))\n",
    "    print('Test RMSE:', np.sqrt(mean_squared_error(y_test,y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eba566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, os, datetime, json\n",
    "os.makedirs('models',exist_ok=True)\n",
    "fname=f\"models/{BASE_MODEL.lower()}_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}.pkl\"\n",
    "joblib.dump(best_model,fname)\n",
    "print('Saved best model âžœ', fname)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
